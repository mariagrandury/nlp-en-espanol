{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifty-ebony",
   "metadata": {},
   "source": [
    "# Fine-Tune XLSR-Wav2Vec2 en Español"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-fountain",
   "metadata": {},
   "source": [
    "Wav2Vec2 es un modelo preentrenado para el reconocimiento automático del habla (Automatic Speech Recognition, ASR) que fue introducido en septiembre de 2020 por Alexei Baevski, Michael Auli y Alex Conneau. Poco después de que se demostrara el rendimiento superior de Wav2Vec2 en el conjunto de datos de ASR en inglés LibriSpeech, Facebook AI presentó XLSR-Wav2Vec2. \"XLSR\" proviene del inglés \"cross-lingual speech representations\" y hace referencia a la capacidad de XLSR-Wav2Vec2 de aprender representaciones del habla que son útiles en varios idiomas.\n",
    "\n",
    "Al igual que Wav2Vec2, XLSR-Wav2Vec2 es capaz de aprender potentes representaciones del lenguaje a partir de cientos de miles de horas de grabaciones en más de 50 idiomas sin etiquetar. De manera similar a BERT, este modelo aprende representaciones del lenguaje contextualizadas enmascarando aleatoriamente los vectores de características antes de pasarlos a una red de Transformers.\n",
    "\n",
    "![XLSR-Wav2Vec2](xlsr_wav2vec2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-oxide",
   "metadata": {},
   "source": [
    "Los autores demostraron por primera vez que el preentrenamiento masivo de un modelo de ASR con grabaciones no etiquetadas en varios idiomas, seguido de un ajuste fino específico para cada idioma con muy pocos datos etiquetados, permite obtener resultados de vanguardia. Véase las Tablas 1-5 del paper original.\n",
    "\n",
    "En este notebook vamos a explicar cómo el checkpoint preentrenado de XLSR-Wav2Vec2 puede ser afinado con un conjunto de datos ASR de cualquier idioma sin necesidad de que este cuente con gran cantidad de horas de grabación. Hay que tener en cuenta que en este notebook vamos a afinar el modelo XLSR-Wav2Vec2 sin usar un modelo de lenguage. Esta manera es más sencilla y eficiente pero se pueden conseguir mejores resultados incluyendo un modelo de lenguaje.\n",
    "\n",
    "A modo de demostración, ponemos a punto el wav2vec2-large-xlsr-53 en el conjunto de datos ASR turco de Common Voice, de escasos recursos, que contiene sólo ~6h de datos de entrenamiento validados.\n",
    "\n",
    "XLSR-Wav2Vec2 se afina utilizando la Clasificación Temporal Conexionista (Connectionist Temporal Classification, CTC), un algoritmo utilizado en el entrenamiento de redes neuronales para problemas Seq2Seq y en particular el reconocimiento automático del habla y de la escritura.\n",
    "\n",
    "Se recomienda encarecidamente la lectura del artículo Sequence Modeling with CTC (2017) de Awni Hannun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-seminar",
   "metadata": {},
   "source": [
    "Antes de empezar, vamos a instalar las librerías datasets y transformers. También necesitamos los paquetes torchaudio y librosa para cargar los archivos de audio y jiwer para evaluar nuestro modelo fine-tuned utilizando la tasa de error de palabras (Word Error Rate, WER).\n",
    "\n",
    "En el paper el modelo se evaluó utilizando la tasa de error de fonemas (Phoneme Error Rate, PER), pero la métrica más común en ASR es la tasa de error de palabras. Teniendo esto en cuenta y con la intención de que este notebook sea lo más general posible, hemos decidido evaluar el modelo utilizando WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==1.4.1\n",
    "!pip install transformers==4.4.0\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
